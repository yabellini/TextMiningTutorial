---
title: "Introducción a Text Mining"
output:
  learnr::tutorial:
    language: es
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
tutorial:
  version: 1.0
description: "Tutorial de introducción al paquete tidytext para trabajar con texto"
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = FALSE)
library(tidytext)
library(dplyr)
library(tidyr)
library(tibble)
library(ggplot2)
library(magrittr)

## Dataset con versos para trabajar en los ejemplos
limericks <- c("Siempre de frac y con zapatos finos,",
              "No parece que fueran argentinos.",
              "por que, por que sera", 
              "Que no usan chiripa", 
              "Ni poncho ni alpargatas los pinguinos?",
              "Un hipopotamo tan chiquitito",
              "Que parezca de lejos un mosquito,",
              "Que se pueda hacer upa",
              "Y mirarlo con lupa,",
              "Debe de ser un hipopotamito",
              "Si un toro, en vez de ser todo de cuero, ",
              "Es de plumas y vuela muy ligero, ",
              "Si tiene dos patitas muy largas y finitas... ",
              "Basta, ya se: no es toro sino tero.",
              "En el medio del mar nada un atun",
              "Estilo mariposa y al tuntun",
              "Nadando a la carrera", 
              "Quizas ganar espera",
              "Si no la maraton, la maratun",
              "Si alguna vez conocen una trucha",
              "Que en un arbol muy alto hizo la cucha,",
              "Que solamente nada",
              "En agua no mojada,",
              "Senores, esa trucha esta enfermucha.",
              "En Tucuman vivia una tortuga",
              "viejisima pero sin una sola arruga,",
              "porque en toda ocasion",
              "tuvo la precaucion",
              "de comer bien planchada la lechuga.")

limericks_df <- tibble(line = 1:29, text = limericks)

limericks_df <- limericks_df %>%
  unnest_tokens(palabra, text)

stopwords_web<- read.csv("https://bitsandbricks.github.io/data/stopwords_es.csv")
```


## El formato tidy para texto

El uso de principios de datos ordenados es una forma poderosa para hacer que el manejo de datos sea más fácil y efectivo. Esto también aplica cuando trabajamos con texto. Los datos ordenados o **tidy** tienen una estructura específica:

  * Cada _variable_ es una _columna_.
  * Cada _observación_ es una _fila_.
  * Cada _tipo de unidad de observación_ es una _tabla_.

Teniendo en cuenta esta definición, el formato de texto ordenado (tidy text) es una _tabla_ con un _token_ por _fila_. 

Un _token_ es una _unidad significativa de texto_, como una _palabra_, que queremos usar para el análisis, y la _tokenización_ es el proceso de _dividir el texto_ en _tokens_. 

Para una minería de texto ordenada, el _token_ que se almacena en cada _fila_ suele ser _una sola palabra_, pero también puede ser un _n-grama_, una _oración_ o un _párrafo_. El paquete `tidytext`, proporciona las funciones para tokenizar por unidades de texto de uso común como las mencionadas y convertirlas a un formato de un término por fila.

Además, contar con el texto en un formato tidy, nos permitirá usar los paquetes del _tidyverse_ para trabajar con los datos.

### Trabajando con texto 

Vamos a conocer las funciones fundamentales para ordenar nuestro texto, sigamos el orden de los siguientes ejercicios para ver como trabaja la función `unnest_tokens`. 

María Elena Walsh escribió un hermoso libro llamado _Zoo Loco_ que contiene _limericks_: versos que riman y no tienen mucho sentido y por eso son muy graciosos.  _Nota: vamos a usar el texto sin acentos para evitar problemas de codificación, pero más adelante veremos como resolverlos_

Ejecuta el siguiente código para generar un _vector de caracteres_ en R con el limericks.



```{r vector_texto, exercise=TRUE}

text <- c("En Tucuman vivia una tortuga",
          "viejisima pero sin una sola arruga,",
          "porque en toda ocasion",
          "tuvo la precaucion",
          "de comer bien planchada la lechuga.")

text
```

### Mensaje

Buen trabajo! Usaremos fragmentos de código interactivos como este en el tutorial. Siempre que encuentres uno, puedes hacer clic en _Ejecutar código_ para ejecutar el código escrito. Si hay un botón _Pista_ o _Solución_, puedes hacer clic para recibir ayuda y/o ver la respuesta.

Este es un vector de caracteres típico que podríamos querer analizar. Para convertirlo en un conjunto de datos de texto ordenado, primero debemos colocarlo en un data frame.  Ejecuta el código para ver el resultado.

```{r convertir_data_frame, exercise = TRUE, exercise.setup = "vector_texto"}

text_df <- tibble(line = 1:5, text = text)

text_df

```

###

Ya tenemos nuestro limerick en un _tibble_, sin embargo, el texto aún no es compatible con un análisis de texto ordenado. No podemos filtrar las palabras ni contar las que ocurren con mayor frecuencia, ya que cada fila está formada por varias palabras combinadas. Necesitamos convertir este texto para que tenga _un token por documento por fila_.

**Un token es una unidad significativa de texto, generalmente una palabra, que queremos en usar para un análisis más detallado, y la tokenización es el proceso de dividir el texto en tokens.**

En este primer ejemplo, solo tenemos un documento (el limerick), pero en general trabajaremos con datos de varios documentos.

Dentro de nuestro tibble, necesitamos dividir el texto en tokens individuales (un proceso llamado tokenización) y transformarlo en una estructura de datos ordenada. Para hacer esto, usamos la función `unnest_tokens()` del paquete `tidytext`.

Ejecuta el siguiente código para ver como trabaja esta función.

```{r tokenizar, exercise = TRUE, exercise.setup = "convertir_data_frame"}

library(tidytext)

text_df %>%
  unnest_tokens(palabra, text)

```
### La función `unnest_tokens`

Buen trabajo!. El código anterior nos permitió usar dos argumentos básicos de la función `unnest_tokens`: 

* El nombre de la columna que se creará cuando el texto se separe en tokens, en este caso el nombre es _palabra_. 
* La columna de entrada de la que proviene el texto: _text_ en este caso. 

Recordemos que _text_df_ creado en el ejercicio anterior tiene una columna llamada _text_ que contiene los datos de interés.

Después de usar `unnest_tokens`, dividimos cada fila original para que genere una fila por cada token (palabra) en el nuevo tibble; la tokenización predeterminada en `unnest_tokens()` es para palabras individuales, como en este ejemplo. La funcion también hace otras tareas por defecto:

  * Se conservan otras columnas, como el número de línea de donde proviene cada palabra.
  * Se elimina la puntuación (las comas no están más, en este ejemplo).
  * Se convierten los tokens a minúsculas, lo que los hace más fáciles de comparar o combinar con otros conjuntos de datos. (El argumento `to_lower = FALSE` desactiva este comportamiento).

Con los datos de texto en este formato ordenado, podemos procesarlos y visualizarlos usando las herramientas estándar para datos tidy, como `dplyr`, `tidyr` y `ggplot2`.

### Ordenando más limericks

Ahora que sabemos usar esta función vamos a ordenar más limericks de Zoo Loco.  Ejecuta el siguiente código para generar el set de datos de limericks que vamos a usar en el resto de los ejercicios:

```{r limericks, exercise = TRUE}

limericks <- c("Siempre de frac y con zapatos finos,",
              "No parece que fueran argentinos.",
              "por que, por que sera", 
              "Que no usan chiripa", 
              "Ni poncho ni alpargatas los pinguinos?",
              "Un hipopotamo tan chiquitito",
              "Que parezca de lejos un mosquito,",
              "Que se pueda hacer upa",
              "Y mirarlo con lupa,",
              "Debe de ser un hipopotamito",
              "Si un toro, en vez de ser todo de cuero, ",
              "Es de plumas y vuela muy ligero, ",
              "Si tiene dos patitas muy largas y finitas... ",
              "Basta, ya se: no es toro sino tero.",
              "En el medio del mar nada un atun",
              "Estilo mariposa y al tuntun",
              "Nadando a la carrera", 
              "Quizas ganar espera",
              "Si no la maraton, la maratun",
              "Si alguna vez conocen una trucha",
              "Que en un arbol muy alto hizo la cucha,",
              "Que solamente nada",
              "En agua no mojada,",
              "Senores, esa trucha esta enfermucha.",
              "En Tucuman vivia una tortuga",
              "viejisima pero sin una sola arruga,",
              "porque en toda ocasion",
              "tuvo la precaucion",
              "de comer bien planchada la lechuga.")

limericks
```
### Mensaje

¡Muy bien!, ahora tenemos un vector con 29 líneas de texto de cuatro limericks de Zoo Loco.

### Transformando los limericks en un tibble

Teniendo en cuenta los ejercicios anteriores, completa el siguiente código para generar un tibble con los datos, prueba ejecutando el código para ver si funcionó. 

```{r convertir_data_frame_limericks, exercise = TRUE, exercise.setup = "limericks"}

limericks_df <- ____________(line = 1:__, text = text)

limericks_df

```

```{r convertir_data_frame_limericks-hint-1}

Recuerda que la función para generar un data frame se llama tibble.
  
```

```{r convertir_data_frame_limericks-hint-2}

El vector limericks tiene 29 filas.
  
```

```{r convertir_data_frame_limericks-solution}

limericks_df <- tibble(line = 1:29, text = limericks)

limericks_df
  
```

### Mensaje

¡Excelente!, Ya convertiste el vector en un tibble, ahora convirtamos el tibble de los limericks en un conjunto de dato ordenados, donde cada palabra sea una fila.  

Completa el siguiente código para realizar esta tarea.

```{r limericks_tidy, exercise = TRUE, exercise.setup = "limericks"}

limericks_df <- tibble(line = 1:29, text = limericks)

limericks_df <- limericks_df %>%
  __________(__________, text)

limericks_df
```

```{r limericks_tidy-hint}
limericks_df <- tibble(line = 1:29, text = limericks)

limericks_df <- limericks_df %>%
  unnest_tokens(_____________, text)

limericks_df
```

```{r limericks_tidy-solution}
limericks_df <- tibble(line = 1:29, text = limericks)

limericks_df <- limericks_df %>%
  unnest_tokens(palabra, text)

limericks_df
```

### Vamos a contar palabras

¡Buen trabajo! En el tibble `limericks_df` ahora tenemos 161 filas, una por cada palabra que se encontraba en los versos.

Si recorremos el tibble veremos que hay algunas palabras que se repiten, para saber cuantas veces aparece una palabra podemos contarlas.

Y como el texto ahora tiene un formato _tidy_ podemos usar los verbos de los paquetes del tidyverse, como por ejemplo la función `count()` para mostrar las palabras más comunes en todos los versos.

Ejecuta el siguiente código para ver cuantas veces aparece cada palabra

```{r cantidad_palabras, exercise = TRUE, exercise.setup = "limericks"}
limericks_df <- tibble(line = 1:29, text = limericks)

limericks_df <- limericks_df %>%
  unnest_tokens(palabra, text)

limericks_df %>%
  count(palabra, sort = TRUE) 

```

### Palabras comunes o stop words

Como podemos ver en el listado las palabras que aparecen más veces son palabras que no son útiles para un análisis, generalmente palabras extremadamente comunes como los artículos y conectores, por ejemplo, "la", "el", "y", "con", en español.

Para poder remover esas palabras, primero necesitamos un data frame que las liste. Ejecuta el siguiente código para descargar un listado de _stop words_ en español del sitio "Bits & Bricks", mantenido por el científico de datos _Antonio Vázquez Brust_

```{r stop_words, exercise = TRUE}

stopwords_web<- read.csv("https://bitsandbricks.github.io/data/stopwords_es.csv")

head(stopwords_web)


```
### 

Ahora que tenemos el listado de stop words, podemos sacarlas del conjunto de palabras de los versos haciendo un `anti_join` entre ambos datasets.

La función `anti_join` necesita como parámetros los dos dataset a unir y luego las columnas por las cuales se realiza la unión.  En el siguiente código, los dataset son _limericks_df_ y _stopwords_web_.  El parámetro `by` recibe los nombres de las columnas, en este caso _cuando_ la _columna palabra_ del dataset de los limericks _sea igual_ a la _columna STOPWORD_ del dataset de las stop word, la fila se elimina del listado.

Ejecuta este código para ver el resultado


```{r anti_join, exercise = TRUE, setup.exercise = "stop_words"}

limericks_df_filtrado <- limericks_df %>% 
  anti_join(stopwords_web, by = c( "palabra" = "STOPWORD"))

limericks_df_filtrado
```

###

Ahora vamos a ver de nuevo nuestra frecuencia de palabras.  Completa el siguiente código para ver ahora cuales son las palabras que más se repiten:

```{r frecuencia_nueva, exercise = TRUE, setup.exercise = "anti_join"}

limericks_df_filtrado <- limericks_df %>% 
  anti_join(stopwords_web, by = c( "palabra" = "STOPWORD"))

limericks_df_filtrado %>%
  _______(palabra, sort = TRUE) 

```

```{r frecuencia_nueva-solution}
limericks_df_filtrado <- limericks_df %>% 
  anti_join(stopwords_web, by = c( "palabra" = "STOPWORD"))

limericks_df_filtrado %>%
  count(palabra, sort = TRUE) 

```

### Mensaje

Perfecto, como vemos en el resultado ahora las palabras que más se repiten son _toro_, _trucha_ y _vez_. 

Como ejercicio final, realizaremos un gráfico de las 10 primeras palabras.  Para hacer eso, ejecuta el siguiente código:

```{r grafico, exercise = TRUE}
limericks_df_filtrado <- limericks_df %>% 
  anti_join(stopwords_web, by = c( "palabra" = "STOPWORD"))

limericks_df_filtrado %>%
  count(palabra, sort = TRUE) %>%
  filter(n > 1) %>%
  mutate(palabra = reorder(palabra, n)) %>%
  ggplot(aes(n, palabra)) +
  geom_col() +
  labs(y = NULL)


```

### 

Recien visualizamos solo las 3 palabras que aparecen más de una vez, modifiquen el código para visualizar todas las palabras y no solo las que aparecen más de una vez:

```{r grafico_2, exercise = TRUE}
limericks_df_filtrado <- limericks_df %>% 
  anti_join(stopwords_web, by = c( "palabra" = "STOPWORD"))

limericks_df_filtrado %>%
  count(palabra, sort = TRUE) %>%
  filter(n > 1) %>%
  mutate(palabra = reorder(palabra, n)) %>%
  ggplot(aes(n, palabra)) +
  geom_col() +
  labs(y = NULL)

```

### Mensaje

Muy bien! solo es necesario sacar el verbo `filter` o bien cambiar el valor 1 por 0 en el código.

Con esto hemos repasado las funciones básicas para transformar texto en texto ordenado, tokenizar un conjunto de datos, remover stop words, calcular frecuencias de palabras y graficarlas.

Para seguir aprendiendo revisa nuestras fuentes en la siguiente sección.

## Fuentes

* Libro Text Minig with R. Julia Silge and David Robinson. https://www.tidytextmining.com/index.html 

* Libro Zoo Loco. María Elena Walsh. https://g.co/kgs/vagz5F

## Licencia

```{r, echo=FALSE, fig.align = "left"}
knitr::include_graphics("images/CC_BY-SA_4.0.png")  
```

Este curso se comparte bajo la licencia [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/deed.es_ES) y fue realizado por [Yanina Bellini Saibene](https://yabellini.netlify.app/)
